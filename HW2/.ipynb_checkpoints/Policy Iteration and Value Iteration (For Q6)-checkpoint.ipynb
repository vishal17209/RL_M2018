{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utitlity functions\n",
    "def isterminal(state, size):\n",
    "    if(state == (0,0) or state == (size-1,size-1)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration Bug Fix\n",
    "\n",
    "As given in the exercise 4.4 of S+B the pseudocode shoudl enter an infinite loop if the policy continuously switches between two or more policies.\n",
    "Which implies that at every step the code chosing two equally optimal actions and thus a fix should be compare the maximum of optimal value function as all optimal policies will lead to the same optimal value function.\n",
    "This bug fix is marked by comments in the code in the Policy Improvement section of PI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP:\n",
    "    #Constructor\n",
    "    def __init__(self, size, actions, policy, discount):\n",
    "        self.size = size\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.policy = policy\n",
    "    \n",
    "    #function to print the policy variable of an object\n",
    "    def printPolicy(self):\n",
    "        for i in self.policy:\n",
    "            print(self.policy[i])\n",
    "        print()\n",
    "        \n",
    "    '''\n",
    "    This function takes current state - state and the action to be performed - action as inputs \n",
    "    and gives the next state and reward according to gridworld problem\n",
    "    '''\n",
    "    def getNextState(self, state, action):\n",
    "        if(isterminal(state, self.size)):\n",
    "            return state, 0\n",
    "        else:\n",
    "            next_state = tuple(np.array(state) + np.array(action))\n",
    "            if(next_state[0] < self.size and next_state[1] < self.size and next_state[0] >= 0 and next_state[1] >= 0):\n",
    "                return next_state, -1\n",
    "            else:\n",
    "                return state, -1\n",
    "        \n",
    "    '''\n",
    "    Function perform Policy iteration\n",
    "    '''\n",
    "    def policy_iteration(self):\n",
    "        \n",
    "        #lists to log the value functions and the policy\n",
    "        value_log = []\n",
    "        policy_log = []\n",
    "        \n",
    "        while(True):\n",
    "            #policy evaluation\n",
    "            values = np.zeros((self.size, self.size))\n",
    "            \n",
    "            #for logging data\n",
    "            iteration = 0\n",
    "            value_log.append([])\n",
    "            \n",
    "            while(True):\n",
    "                new_v = np.copy(values)\n",
    "                delta = 0\n",
    "                \n",
    "                '''\n",
    "                Iterating over all the states and then over all actions to get the matrix of value functions\n",
    "                and then repeating the process to evaluate the optimal value function\n",
    "                '''\n",
    "                for i in range(self.size):\n",
    "                    for j in range(self.size):\n",
    "                        state = (i,j)\n",
    "                        v = 0\n",
    "                        for a in range(len(self.actions)):\n",
    "                            new_state, reward = self.getNextState(state, self.actions[a])\n",
    "                            v += self.policy[state][a]*(reward + self.discount*values[new_state[0], new_state[1]])\n",
    "                            \n",
    "                        #Calcuating the difference between new and old values\n",
    "                        delta = max(delta, np.abs(v - new_v[i,j]))\n",
    "                        new_v[i,j] = v\n",
    "                        \n",
    "                #updating value function\n",
    "                values = new_v\n",
    "                \n",
    "                #logging data\n",
    "                value_log[iteration].append(values)\n",
    "                \n",
    "                #Break when converged\n",
    "                if(delta < 1e-5):\n",
    "                    break\n",
    "            \n",
    "            #policy improvement\n",
    "            policy_stable = True\n",
    "            \n",
    "            '''\n",
    "            Iterating over all the states and then calculating action values using the old policy and the new policy\n",
    "            obtained by Policy evaluation and then stopping if both the policy and the value function remains the same(fix for bug in ex4.4)\n",
    "            '''\n",
    "            for i in range(self.size):\n",
    "                for j in range(self.size):\n",
    "                    action_values = np.zeros(len(self.actions))\n",
    "                    state = (i,j)\n",
    "                    for a in range(len(self.actions)):\n",
    "                        new_state , reward = self.getNextState(state, self.actions[a])\n",
    "                        action_values[a] += (reward + self.discount*values[new_state[0], new_state[1]])\n",
    "                        \n",
    "                    #Getting the old and new action using argmax\n",
    "                    old_action = np.argmax(self.policy[state])\n",
    "                    new_action = np.argmax(action_values)\n",
    "                        \n",
    "                    #Bug Fix\n",
    "                    #Getting the old and new optimal values\n",
    "                    old_value = np.max(self.policy[state])\n",
    "                    new_value = np.max(action_values)\n",
    "            \n",
    "                    # (Bug Fix)Stopping if the policy and value function didnt change as they will always become better otherwise\n",
    "                    if(old_action != new_action and old_value != new_value):\n",
    "                        policy_stable = False\n",
    "                    \n",
    "                    #Updating policy\n",
    "                    temp = np.zeros(len(self.actions))\n",
    "                    temp[new_action] = 1\n",
    "                    self.policy[state] = temp \n",
    "                    \n",
    "                    policy_log.append(self.policy)\n",
    "            \n",
    "            #returning policy when its stable\n",
    "            if(policy_stable):\n",
    "                return self.policy, policy_log, value_log\n",
    "                        \n",
    "    '''\n",
    "    Function to perform Value Iteration\n",
    "    '''\n",
    "    def value_iteration(self):\n",
    "        \n",
    "        #for logging data\n",
    "        value_log = []\n",
    "        values = np.zeros((self.size,self.size))\n",
    "        \n",
    "        while(True):\n",
    "            delta = 0\n",
    "            new_v = np.copy(values)\n",
    "            \n",
    "            '''\n",
    "            Iterating over all states and actions and calculating value function and then\n",
    "            updating the old value function with the optimal value function\n",
    "            '''\n",
    "            for i in range(self.size):\n",
    "                for j in range(self.size):\n",
    "                    state = (i,j)\n",
    "                    v = new_v[i,j]\n",
    "                    actions_values = np.zeros(len(self.actions))\n",
    "                    for a in range(len(self.actions)):\n",
    "                        new_state, reward = self.getNextState(state, self.actions[a])\n",
    "                        actions_values[a] += (reward + self.discount*values[new_state[0], new_state[1]])\n",
    "                    \n",
    "                    #Updating with the new optimal value function\n",
    "                    new_v[i,j] = np.max(actions_values)\n",
    "                    delta = max(delta, np.abs(new_v[i,j] - v))\n",
    "                    \n",
    "            #updating and logging values\n",
    "            values = new_v\n",
    "            value_log.append(values)\n",
    "            if(delta < 1e-4):\n",
    "                break\n",
    "        \n",
    "        #Calculating the optimal policy using the optimal value function obtained above\n",
    "        policy = np.zeros((self.size, self.size))\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                state = (i,j)\n",
    "                action_values = np.zeros(len(self.actions))\n",
    "                for a in range(len(self.actions)):\n",
    "                    new_state, reward = self.getNextState(state, self.actions[a])\n",
    "                    action_values[a] += (reward + self.discount*values[new_state[0], new_state[1]])\n",
    "                policy[i,j] = np.argmax(action_values)\n",
    "        return policy, value_log\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (0 = up, 1 = right, 2 = left, 3 = left)\n",
      "0 3 3 2 \n",
      "0 0 0 2 \n",
      "0 0 1 2 \n",
      "0 1 1 0 \n",
      "\n",
      "Few steps of Policy Evaluation for Iteration 1: \n",
      "Evaluation iteration number:  0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "Evaluation iteration number:  1\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "\n",
      "Evaluation iteration number:  2\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "\n",
      "Few Steps of Policy Improvement: \n",
      "For iteration  0\n",
      "0 3 3 2 \n",
      "0 0 0 2 \n",
      "0 0 1 2 \n",
      "0 1 1 0 \n",
      "\n",
      "For iteration  1\n",
      "0 3 3 2 \n",
      "0 0 0 2 \n",
      "0 0 1 2 \n",
      "0 1 1 0 \n",
      "\n",
      "For iteration  2\n",
      "0 3 3 2 \n",
      "0 0 0 2 \n",
      "0 0 1 2 \n",
      "0 1 1 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_policy = {}\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        fake_policy[(i,j)] = np.ones(4)/4\n",
    "        \n",
    "dp = DP(4, [(-1,0),(0,1),(1,0),(0,-1)], fake_policy, 1)\n",
    "\n",
    "policy, pl, vl = dp.policy_iteration()\n",
    "\n",
    "print(\"Optimal Policy (0 = up, 1 = right, 2 = left, 3 = left)\")\n",
    "temp = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(np.argmax(policy[(i,j)]), end=\" \")\n",
    "    print()\n",
    "print()\n",
    "\n",
    "print(\"Few steps of Policy Evaluation for Iteration 1: \")\n",
    "for i in range(3):\n",
    "    print(\"Evaluation iteration number: \", i)\n",
    "    print(vl[0][i])\n",
    "    print()\n",
    "    \n",
    "print(\"Few Steps of Policy Improvement: \")\n",
    "for k in range(3):\n",
    "    print(\"For iteration \", k)\n",
    "    temp = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            print(np.argmax(pl[k][i,j]), end=\" \")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (0 = up, 1 = right, 2 = left, 3 = left)\n",
      "0 3 3 2 \n",
      "0 0 0 2 \n",
      "0 0 1 2 \n",
      "0 1 1 0 \n",
      "\n",
      "Few Steps of Values Iteration\n",
      "For iteration  0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "For iteration  1\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp_v = DP(4, [(-1,0),(0,1),(1,0),(0,-1)], fake_policy, 1)\n",
    "policy_v , vvl = dp_v.value_iteration()\n",
    "\n",
    "print(\"Optimal Policy (0 = up, 1 = right, 2 = left, 3 = left)\")\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(int(policy_v[i,j]), end=\" \")\n",
    "    print()\n",
    "print()\n",
    "\n",
    "print(\"Few Steps of Values Iteration\")\n",
    "for i in range(3):\n",
    "    print(\"For iteration \", i)\n",
    "    print(vvl[i])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
