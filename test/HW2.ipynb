{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self):\n",
    "        self.size = 5\n",
    "        self.a = (0,1)\n",
    "        self.b = (0,3)\n",
    "        self.a_prime = (4,1)\n",
    "        self.b_prime = (2,3)\n",
    "        self.actions = [(0,1), (1,0), (0,-1), (-1,0)]\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        if(state == self.a):\n",
    "            return self.a_prime, 10\n",
    "        if(state == self.b):\n",
    "            return self.b_prime, 5\n",
    "        next_state = tuple(np.array(state) + np.array(action))\n",
    "        if(next_state[0] < 0 or next_state[1] < 0 or next_state[0] >= self.size or next_state[1] >= self.size):\n",
    "            return state, -1\n",
    "        return next_state, 0\n",
    "        \n",
    "    def figure32(self):\n",
    "        m = np.zeros((self.size, self.size), dtype=int)\n",
    "        c = 0\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                m[i,j] = c\n",
    "                c+=1\n",
    "            \n",
    "        A = np.zeros((self.size**2, self.size**2))\n",
    "        b = np.zeros(self.size**2)\n",
    "        \n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                state = (i,j)\n",
    "                for a in range(len(self.actions)):\n",
    "                    (next_i, next_j), reward = self.step(state, self.actions[a])\n",
    "#                     print(m[i,j], m[next_i, next_j])\n",
    "                    A[m[i,j], m[next_i, next_j]] += 0.25*0.9\n",
    "                    b[m[i,j]] += 0.25*reward\n",
    "                A[m[i,j], m[i,j]] = A[m[i,j], m[i,j]]-1\n",
    "        \n",
    "        X = np.linalg.solve(A,b)\n",
    "        X = np.round(X, 1)\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                print(X[m[i,j]], end=\" \")\n",
    "            print()\n",
    "            \n",
    "class Iterations:\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.terminal = [(0,0), (3,3)]\n",
    "        self.actions = [(0,1), (1,0), (0,-1), (-1,0)]\n",
    "        self.policy = np.ones((self.size**2, len(self.actions)))/4\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        if(state in self.terminal):\n",
    "            return state, 0\n",
    "        next_state = tuple(np.array(state) + np.array(action))\n",
    "        if(next_state[0] < 0 or next_state[1] < 0 or next_state[0] >= self.size or next_state[1] >= self.size):\n",
    "            return state, -1\n",
    "        return next_state, -1\n",
    "        \n",
    "    def policy_iteration(self):\n",
    "        m = np.zeros((self.size, self.size), dtype=int)\n",
    "        c = 0\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                m[i,j] = c\n",
    "                c+=1\n",
    "                \n",
    "        value_func = None\n",
    "        while(True):\n",
    "            value_func = np.zeros((self.size, self.size))\n",
    "            #evaluation\n",
    "            while(True):\n",
    "                delta = 0\n",
    "                new_v = np.copy(value_func)\n",
    "                for i in range(self.size):\n",
    "                    for j in range(self.size):\n",
    "                        state = (i,j)\n",
    "                        value = 0\n",
    "                        for a in range(len(self.actions)):\n",
    "                            next_state, reward = self.step(state, self.actions[a])\n",
    "                            value += self.policy[m[i,j],a]*(reward + value_func[next_state[0], next_state[1]])\n",
    "                        delta = max(delta, abs(value-value_func[i,j]))\n",
    "                        value_func[i,j] = value\n",
    "#                 print(value_func)\n",
    "#                 print(delta)\n",
    "#                 value_func = new_v\n",
    "                if(delta < 1e-4):\n",
    "                    break\n",
    "                \n",
    "#             print(value_func)\n",
    "            #improvement\n",
    "            policy_stable = True\n",
    "            new_policy = []\n",
    "            for i in range(self.size):\n",
    "                for j in range(self.size):\n",
    "                    state = (i,j)\n",
    "                    action_values = np.zeros(len(self.actions))\n",
    "                    for a in range(len(self.actions)):\n",
    "                        next_state, reward = self.step(state, self.actions[a])\n",
    "                        action_values[a] += (reward + value_func[next_state[0], next_state[1]])\n",
    "                    \n",
    "                    old_action = np.argmax(self.policy[m[i,j]])\n",
    "                    new_action = np.argmax(action_values)\n",
    "                    \n",
    "                    if(old_action != new_action):\n",
    "                        policy_stable = False\n",
    "                    \n",
    "                    temp = np.zeros(len(self.actions), dtype=int)\n",
    "                    temp[new_action] = 1\n",
    "                    self.policy[m[i,j]] = temp\n",
    "                    \n",
    "            if(policy_stable):\n",
    "                break\n",
    "                \n",
    "        print(self.policy)\n",
    "        print(value_func)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.3 -8.8 -4.4 -5.3 -1.5 \n",
      "-1.5 -3.0 -2.3 -1.9 -0.5 \n",
      "-0.1 -0.7 -0.7 -0.4 0.4 \n",
      "1.0 0.4 0.4 0.6 1.2 \n",
      "1.9 1.3 1.2 1.4 2.0 \n"
     ]
    }
   ],
   "source": [
    "mdp = MDP()\n",
    "mdp.figure32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "it = Iterations()\n",
    "it.policy_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
